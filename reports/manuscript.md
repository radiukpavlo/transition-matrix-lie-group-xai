# Equivariant Transition Matrices for Explainable Deep Learning: A Lie Group Linearization Approach

## 1. Introduction

The rapid development of deep learning (DL) has led to the introduction of highly effective models in critical areas such as medicine, forensics, and autonomous systems. However, the "black box" nature of these models remains a fundamental barrier to their widespread adoption, as developers and stakeholders require not only accuracy but also transparency and trust [1,2]. Explainable Artificial Intelligence (XAI) emerged as a necessary field designed to bridge the gap between complex Formal Models (FM) and human Mental Models (MM) [3].

In our previous research [4,5], we proposed a visual analytics approach based on transition matrices to map the high-dimensional feature space of a DL model onto an interpretable machine learning (ML) space. Although this approach demonstrated effectiveness for classification tasks, it is implicitly based on the assumption that the transition between two feature representations can be described by a linear mapping—that is, the relationship between feature spaces is linear or sufficiently close to linear in the data's operating region. In reality, the relationships between the latent features of a deep neural network and human-understandable attributes are often non-linear [6]. Importantly, these non-linearities are not random: in many tasks, they reflect the presence of hidden geometric structures and symmetries in the data, i.e., actions of continuous groups of transformations, which at the object level may be realized as complex (non-linear) deformations [7], but simultaneously possess a controllable local structure that can be described by Lie algebra generators.

In this context, a natural way to constrain non-linearity is not to attempt to approximate it with an arbitrary function, but to require structural consistency with the geometry of the data. This is why recent results in geometric deep learning emphasize the importance of equivariance—a property where a transformation in the input space causes a predictable transformation in the feature space [8]; meaning the non-linear relationship between representations is subject to the action of symmetries rather than being arbitrary.

However, most XAI methods do not impose explicit constraints on symmetry consistency. If a deep learning model is sensitive to a specific symmetry variation of an object (meaning its internal features change consistently with the action of transformations in the input space), then the interpreted surrogate model must reproduce this change structurally correctly: identical transformations of the object should lead to a predictable and mathematically consistent transformation of the explainable features. Transition matrices constructed under the implicit assumption of global linearity of the link between feature representations are unable to adequately account for structurally determined non-linear effects caused by symmetries and data geometry; consequently, the obtained explanations may lose interpretational content or become internally inconsistent even with small transformations in the input space.

To eliminate this limitation, this work presents an improved approach to model explainability via the linearization of induced equivariant group actions in feature spaces. In our formulation, the key non-linearity is associated not with the transition matrix itself, but with the continuous action of the Lie group on objects and the induced action in feature spaces; it is this action that we linearize via the Lie algebra.

Using the apparatus of Lie groups and their Lie algebras, we move from selecting a transition matrix solely based on the criterion of matching feature values to structural alignment of two representation spaces: the latent feature space (formal model) and the interpretable feature space (mental model). This allows for the construction of a transition matrix that not only ensures high fidelity of translation between representations but is also compatible with the infinitesimal action of symmetries, i.e., it respects Lie algebra generators in both spaces (for example, the generator of rotations for $SO(2)$).

The hypothesis of the study is the assumption that the connection between deep learning feature spaces and interpreted mental models can be more accurately and reliably represented by a transition matrix that satisfies the **intertwining condition** for symmetry group actions. By linearizing group actions in both spaces via Lie algebra generators, we obtain a structural mapping resistant to variations generated by data symmetries (not only geometric but also other domain-specific transformations described by group actions), thereby providing more reliable and internally consistent explanations compared to approaches using only a fixed linear translation without considering symmetry structure.

The key contributions of this study are as follows:

1. A method is proposed for transforming formal model results into mental model features while preserving the structural symmetries of the data.
2. A methodology is introduced for calculating infinitesimal generators for both formal and mental models, allowing the use of linear intertwining relationships even when the corresponding transformations at the object level are non-linear.
3. A robust mathematical framework based on Singular Value Decomposition (SVD) is proposed to find the optimal transition matrix that balances fidelity and equivariance (structural correspondence).

The remainder of this article is organized as follows. Section 2, "Related Works," analyzes the current state of research in Explainable AI (XAI), geometric deep learning, and the application of transition matrices. Section 3, "Materials and Methods," lays out the mathematical foundation of the method based on Lie algebra theory, details the linearization algorithm, and the construction of the equivariant transition matrix; it also provides a numerical example on synthetic data to validate the mathematical apparatus and describes the experimental methodology on the MNIST dataset. Section 4, "Results," is dedicated to the analysis of obtained data and a quantitative comparison of the robustness of the proposed approach against existing basic methods. Section 5, "Discussion," discusses the theoretical and practical implications of the results, as well as the limitations of the method. Finally, the "Conclusions" section summarizes the contribution of the work and outlines prospective directions for future research.

## 2. Related Works

Current research in Explainable Artificial Intelligence (XAI) and Geometric Deep Learning (GDL) aims to overcome the limitations of traditional deep learning (DL) models, such as their "black box" nature and lack of consideration for data symmetries [1,2,3]. This section reviews key works related to transition matrices for explainability, equivariant neural networks, Lie groups and their algebras, as well as the integration of symmetries in XAI. Special attention is given to the continuation of the authors' previous research, where a basic approach to transition matrices for visual analytics of DL models was proposed [4,5].

In the context of XAI, traditional methods such as SHAP, LIME, and Integrated Gradients focus on local explanations but often ignore the global structure of data, including symmetries [6,7,8]. Deep models require mechanisms that ensure the stability of explanations under geometric transformations, such as rotations or shifts [9,10]. Geometric Deep Learning (GDL) offers a unified framework for accounting for symmetries through group equivariance, where models maintain invariance or equivariance regarding group actions, such as $SO(3)$ for rotations [11,12]. This allows models to more effectively process non-Euclidean data, such as graphs or 3D structures, and improves generalization [13,14].

Equivariant Neural Networks (ENN) are a key element of GDL, where symmetries are integrated directly into the architecture [15,16]. For example, works on Lie groups have proposed frameworks for equivariant networks on reductive Lie groups, generalizing convolutional layers for arbitrary symmetries, including Lorentz or unitary groups [17,18]. Other studies focus on Lie algebras for network canonicalization, allowing equivariance without full knowledge of the group structure [19,20]. Such approaches are applied in physics and chemistry, where symmetries (e.g., rotations and translations) are critical for molecular modeling [21,22]. In the context of XAI, algebraic attacks on explainable models using Lie groups highlight the vulnerability of traditional methods to symmetries, offering geometric perspectives for robustness [23,24].

Transition matrices were proposed for DL explainability in the authors' previous works [25,26]. In [4], visual analytics with transition matrices was developed to map DL model features to interpreted ML model spaces using HITL and SVD for classification tasks on MNIST, FNC-1, and Iris datasets. In [5], this approach was adapted for medical data (ECG and MRI), where transition matrices translate DL decisions into user features compatible with clinical guidelines, achieving high consistency with expert annotations (Cohen’s Kappa 0.89 and 0.80). However, these methods are based on linear approximations and do not account for non-linear data symmetries, leading to instability under geometric perturbations [27,28].

Other works with transition matrices in XAI are limited but include transition phases for estimating losses in DL [23], or fuzzy matrices for estimating interventions in social networks [23]. In GDL, symmetries via Lie groups are applied for equivariant CNNs with Laplace distributions [2], or for quantum networks [5,13]. However, the integration of transition matrices with Lie algebras for XAI remains insufficiently researched [8,28].

To illustrate the gap in the literature, **Table 1** compares existing methods with the proposed approach, considering the research hypothesis: the connection between DL and MM feature spaces can be accurately represented by a transition matrix that satisfies the intertwining condition for symmetry group actions, with linearization via Lie algebras for stability under geometric transformations.

Table 1. Comparison of existing methods with the proposed approach

| Aspect | Existing Methods (XAI, GDL, ENN) | Approach of This Work | Gap |
| :--- | :--- | :--- | :--- |
| **Symmetry Consideration** | Mostly ignore non-linear symmetries; focus on local explanations or basic groups (e.g., $SO(3)$) [1,6,10] | Equivariant transition matrix with Lie algebras for structural consistency [7,15] | Lack of symmetry integration in global explanations; instability to geometric perturbations |
| **Linearization of Non-linear Links** | Purely linear approximations in transition matrices [4,5] | Linearization via infinitesimal Lie group generators [8,16] | Limited accuracy in non-linear data; lack of "respect" for symmetries |
| **Explainability in Critical Spheres** | Local methods without global stability [2,9] | Structural consistency for consistent explanations in medicine, physics [3,17] | Insufficient stability of explanations under data transformations |
| **Computational Stability** | Dependence on data without symmetries [11,18] | SVD with a weight coefficient to balance fidelity and equivariance [12,19] | Overdetermined systems without symmetry regularization |
| **Application in Biology and Chemistry** | Limited use of symmetries for molecular modeling [13,20] | Integration with diffusion models for structure generation [14,21] | Lack of equivariant explanations for biomedical data |
| **Network Universality** | Focus on specific groups without generalization [22,25] | Universal classes of equivariant networks [23,26] | Limited generalization to arbitrary symmetries |
| **Practical Applications** | Theoretical frameworks without extensive testing [24,27] | Practical algorithms for real data [28] | Insufficient empirical confirmation of stability |

The aim of the work is to construct a transition matrix between the latent feature space of a formal model and the space of interpreted features, which ensures stable and internally consistent explanations under the action of data symmetries, by linearizing induced group transformations in feature spaces using the apparatus of Lie groups and their Lie algebras.

### Research Objectives are as follows

1. To develop an approach for constructing a transition matrix between the formal model's latent feature space and the mental model's interpreted feature space, which aligns with the common symmetry structure of the data and ensures structurally correct "translation" between representations.
2. To propose and implement a methodology for empirical estimation of infinitesimal generators of symmetry group action in the latent feature space and the interpreted feature space, allowing the formulation and use of linear intertwining relationships for the transition matrix even in cases where object-level transformations are non-linear.
3. To propose a robust mathematical apparatus based on Singular Value Decomposition (SVD) to search for an optimal transition matrix that balances fidelity and equivariance (structural correspondence to symmetries).

## 3. Materials and Methods

The work proposes an extended approach to Explainable AI, in which the transition between the latent feature space of a deep (formal) model and the space of interpreted (mental) features is built not only on the fidelity criterion but with explicit consideration of data symmetries. The key step is the linearization of the induced Lie group action in both feature spaces via Lie algebra generators and the subsequent construction of a transition matrix compatible with this symmetry structure. Unlike the basic approach [4], where the transition matrix is estimated solely as a linear mapping between two spaces, the proposed formulation additionally accounts for internal data symmetry, assuming that some Lie group of transformations acts on the data.

### 3.1. Problem Formalization

Consider the data space $X$ and let $A \in \mathbb{R}^{m \times k}$ be the matrix of data features obtained from the convolutional layers of a deep neural network (Formal Model, FM), where $m$ is the number of samples, and $k$ is the dimensionality of the deep feature space. In parallel, consider the matrix $B \in \mathbb{R}^{m \times l}$ of corresponding interpreted features (Mental Model, MM), where $l$ is the number of parameters having clear semantics for an expert. For a sample $x \in X$, denote its corresponding rows in matrices $A$ and $B$ as $a(x) \in \mathbb{R}^k$ and $b(x) \in \mathbb{R}^l$.

The sought transition matrix $T \in \mathbb{R}^{l \times k}$ is treated as a linear operator mapping deep features to interpreted ones. At the level of all samples, this is given by the approximate equality:
$$B \approx A T^\top. \quad (1)$$

Unlike standard XAI approaches, which establish only a static correspondence between feature spaces, we additionally impose a hypothesis of structural consistency with data symmetries: for an object $x \in X$, its representations $a(x)$ and $b(x)$ are not independent statistical entities but must transform consistently under the action of symmetries.

More precisely, we assume that a certain Lie group $G$ acts locally on the space $X$,
$$G \curvearrowright X, \quad (g, x) \mapsto g \cdot x,$$
and the representations $a: X \to \mathbb{R}^k$ and $b: X \to \mathbb{R}^l$ are at least approximately equivariant regarding this action. That is, there exist linear representations (i.e., homomorphisms into groups of non-degenerate matrices)
$$\rho_A: G \to GL(k), \quad \rho_B: G \to GL(l),$$
such that for all $g \in G$ and $x \in X$, the following holds:
$$a(g \cdot x) \approx \rho_A(g) a(x), \quad b(g \cdot x) \approx \rho_B(g) b(x).$$

Intuitively, this means: if an object changes only by a "permitted" symmetry $g$, then both feature spaces change consistently, albeit in different coordinate systems. In such a situation, it is natural to demand that the translation matrix $T$ be an **Intertwiner** of the induced group actions, i.e.,
$$T \rho_A(g) \approx \rho_B(g) T, \quad \forall g \in G.$$

This condition is standard in group representation theory and formalizes the assertion that $T$ "respects symmetries" and does not destroy information about how the group acts on the data.

In machine learning terms, a similar idea is realized by equivariant layers: for example, convolution in convolutional networks is an equivariant transformation with respect to shifts, meaning it transfers the symmetry structure between layers, preserving the relevant content of the representation.

Let $\mathfrak{g}$ be the Lie algebra of the $r$-parametric Lie group $G$, $\{\xi_i\}_{i=1}^r$ be a basis of infinitesimal generators, and $J_i^A \in \mathbb{R}^{k \times k}$ and $J_i^B \in \mathbb{R}^{l \times l}$ be the corresponding matrices of differentials of representations $d\rho_A(\xi_i)$ and $d\rho_B(\xi_i)$ in the feature spaces. Then the consistency of symmetry action in linearized form is written as:
$$T J_i^A \approx J_i^B T, \quad i = 1, \dots, r. \quad (2)$$

Thus, the matrix $T$ must satisfy two fundamental requirements: mapping accuracy, ensuring proximity (1), and structural consistency with symmetries, given by relations (2).

In practice, for each sample $x_j$, we build several slightly modified copies
$$x_{j,i} = \exp(\varepsilon \xi_i) \cdot x_j,$$
where $\varepsilon > 0$ is a small step, and $\xi_i$ is a "symmetry direction" (e.g., a very small rotation, shift, or scaling). After that, we calculate features for the original and copies: $a(x_j), a(x_{j,i})$ and $b(x_j), b(x_{j,i})$, and estimate their small changes (finite differences):
$$\Delta a_{j,i} = \frac{a(x_{j,i}) - a(x_j)}{\varepsilon}, \quad \Delta b_{j,i} = \frac{b(x_{j,i}) - b(x_j)}{\varepsilon}.$$
These values show exactly how features react to an "almost imperceptible" symmetric perturbation of the data.

The most important step is that we recover matrices $J_i^A$ and $J_i^B$, which can be interpreted as *linear rules of local feature reaction to symmetry $\xi_i$*:
$$\Delta a_{j,i} \approx J_i^A a(x_j), \quad \Delta b_{j,i} \approx J_i^B b(x_j).$$

After this, we search for matrix $T$ by approximate methods so that it satisfies two requirements simultaneously:

* **Translation Quality:** $T$ must well reproduce $b(x)$ from $a(x)$ on the data.
* **Symmetry Alignment:** If a "small symmetry step" occurred in the latent feature space, then a corresponding "small symmetry step" in their sense must occur after translation into interpreted features, meaning the translation must be consistent with the local action of symmetries.

As a result, we obtain a translation $T$ that is not only accurate on the data but also robust to natural variations generated by symmetries. This increases the reliability of the explanation: interpreted features reflect not a random fit, but a structure consistent with the data geometry.

#### 3.1.1. Example: Rotation Group of the Plane $SO(2)$

As a typical example of symmetries, consider the group of plane rotations centered at the origin:
$$SO(2) = \left\{ R(\theta) = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} : \theta \in \mathbb{R} \right\}.$$
It acts on points of the plane $x \in \mathbb{R}^2$ by the rule:
$$SO(2) \curvearrowright \mathbb{R}^2, \quad (\theta, x) \mapsto R(\theta)x.$$
The Lie algebra $\mathfrak{so}(2)$ is one-dimensional and generated by the matrix:
$$\xi = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}, \quad \text{so that } R(\theta) = \exp(\theta \xi).$$

If feature representations $a(x)$ and $b(x)$ are equivariant with respect to this action, then there exist representations
$$\rho_A: SO(2) \to GL(k), \quad \rho_B: SO(2) \to GL(l),$$
for which the equivariance condition holds:
$$a(R(\theta)x) \approx \rho_A(R(\theta)) a(x), \quad b(R(\theta)x) \approx \rho_B(R(\theta)) b(x).$$

The corresponding linearized (infinitesimal) form of this condition is given through differentials:
$$J^A = d\rho_A(\xi) \in \mathbb{R}^{k \times k}, \quad J^B = d\rho_B(\xi) \in \mathbb{R}^{l \times l},$$
and is written as the intertwining condition for the transition matrix:
$$T J^A \approx J^B T.$$

Introducing the equivariance condition transforms the matrix $T$ from a standard regression coefficient into a structural bridge between models. For example, if a certain combination of neurons in the deep FM model is responsible for recognizing the object's tilt, then the equivariance condition forces matrix $T$ to translate this "deep" tilt precisely into the corresponding "mental" tilt parameter in the MM model. This allows for achieving consistency of explanations: the model will issue similar interpretations for an object and its slightly modified (e.g., rotated) copy, which is critically important for user trust in the XAI system.

### 3.2. Combined System and SVD Solution

Since the sought transition matrix $T$ enters the equations as an operator, standard methods for solving systems like $Ax=b$ cannot be applied directly to matrix equations. To solve this problem, we apply the vectorization operation, which allows converting matrix $T$ into a vector $u = vec(T) \in \mathbb{R}^{kl}$.

The sought operator $T$ must ensure a compromise between approximation accuracy (Fidelity) and preservation of structural correspondence (Equivariance). Formally, the task of finding the optimal transition matrix reduces to minimizing the combined functional:
$$\mathcal{L}(T) = \| B^\top - T A^\top \|_F^2 + \lambda \sum_{i=1}^r \| T J_i^A - J_i^B T \|_F^2 \to \min_T, \quad (3)$$
where $\|\cdot\|_F$ is the Frobenius norm, and $\lambda \ge 0$ is a weight coefficient regulating the influence of structural constraints.

Using the property $vec(AXB) = (B^\top \otimes A)vec(X)$, we transform the fidelity and equivariance conditions into a single system of linear algebraic equations $M \cdot u = Y$:

1. **Fidelity Equation:** The condition $B^\top \approx T A^\top$ transforms into:
    $$(A \otimes I_l) vec(T) = vec(B^\top),$$
    where $I_l$ is the identity matrix of size $l \times l$ (number of mental features), and $\otimes$ is the Kronecker product.

2. **Symmetry Equation (Equivariance):** The condition $T J_i^A - J_i^B T \approx 0$ for each generator $\xi_i$ transforms into:
    $$\left( (J_i^A)^\top \otimes I_l - I_k \otimes J_i^B \right) vec(T) = 0,$$
    where $I_k$ is the identity matrix of size $k \times k$ (dimensionality of deep features).

To form a block matrix, we stack (combine) these equations into a global matrix $M$. To balance between approximation accuracy and structural correspondence, the weight coefficient $\lambda$ is introduced:
$$M = \begin{bmatrix} A \otimes I_l \\ \lambda \cdot K_1 \\ \vdots \\ \lambda \cdot K_r \end{bmatrix}, \quad Y = \begin{bmatrix} vec(B^\top) \\ 0 \\ \vdots \\ 0 \end{bmatrix},$$
where $K_i = (J_i^A)^\top \otimes I_l - I_k \otimes J_i^B$.

Since the obtained system $M \cdot u = Y$ is significantly overdetermined ($ml + rkl$ equations for $kl$ unknowns) and may be noisy, following the approach proposed in [4], we use SVD decomposition (Singular Value Decomposition) of matrix $M$ [9] to find a stable solution:
$$M = U \Sigma V^\top.$$

The solution is found via the Moore-Penrose pseudoinverse matrix $M^+$:
$$u = V \Sigma^+ U^\top Y,$$
where $\Sigma^+$ is a matrix whose diagonal elements are inverses of non-zero singular values $\sigma_i$ (provided $\sigma_i > \tau$, where $\tau$ is the regularization threshold).

After finding the solution $u \in \mathbb{R}^{kl}$, the **reshape** operation is performed, which restores the two-dimensional structure of the transition matrix $T \in \mathbb{R}^{l \times k}$. This step is the reverse operation to vectorization (de-vectorization), returning the found parameters into the form of a linear operator necessary for the final mapping of spaces $B \approx A T^\top$ and subsequent visual analysis of the model.

Using SVD decomposition in this task is critically important because:

* It allows efficient handling of situations where matrix $M$ has incomplete rank (e.g., due to redundancy of FM features).
* Small singular values corresponding to noise in data can be truncated to regularize the solution.
* This ensures obtaining a single optimal (in the least squares sense) vector $u$, which after the reshape operation gives the final extended transition matrix $T$.

### 3.3. Algorithm for Obtaining the Extended Transition Matrix

Below is the algorithm for calculating matrix $T$, which integrates approximation and symmetry conditions via SVD decomposition.

#### Algorithm 1: Calculation of Structurally-Aligned Transition Matrix

##### Input data

* FM feature matrix: $A \in \mathbb{R}^{m \times k}$
* MM feature matrix: $B \in \mathbb{R}^{m \times l}$
* Set of generators for FM: $\{J_1^A, ..., J_r^A\}$, where $J_i^A \in \mathbb{R}^{k \times k}$
* Set of generators for MM: $\{J_1^B, ..., J_r^B\}$, where $J_i^B \in \mathbb{R}^{l \times l}$
* Symmetry weight coefficient: $\lambda$

##### Step 1: Vectorization of the main task (Fidelity)

1.1. Form the system matrix $M_{fid} = (A \otimes I_l) \in \mathbb{R}^{(ml) \times (kl)}$.
1.2. Form the target vector $Y_{fid} = vec(B^\top) \in \mathbb{R}^{(ml)}$.

##### Step 2: Vectorization of structural constraints (Equivariance)

2.1. For each $i \in \{1, ..., r\}$: form the constraint matrix:
$$L_i = \left( (J_i^A)^\top \otimes I_l - I_k \otimes J_i^B \right) \in \mathbb{R}^{(kl) \times (kl)}$$
2.2. Combine all $L_i$ into a vertical block: $M_{sym} = [L_1; ...; L_r] \in \mathbb{R}^{(rkl) \times (kl)}$.
2.3. Form a zero target vector: $Y_{sym} = 0 \in \mathbb{R}^{rkl}$.

##### Step 3: Forming the extended system

3.1. Compose the combined matrix: $\begin{bmatrix} M_{fid} \\ \lambda M_{sym} \end{bmatrix}$.
3.2. Compose the combined vector: $\begin{bmatrix} Y_{fid} \\ Y_{sym} \end{bmatrix}$.

##### Step 4: Solution via SVD (Pseudoinverse)

4.1. Perform SVD decomposition of matrix $M$: $M = U \Sigma V^\top$.
4.2. Calculate the pseudoinverse matrix: $M^+ = V \Sigma^+ U^\top$ (where $\Sigma^+$ contains inverse values of singular numbers $\sigma_i > \tau$).
4.3. Find the parameter vector: $u = M^+ Y$.

##### Step 5: Reconstruction of the transition matrix

5.1. Perform **reshape** operation for vector $u$, converting it to matrix $T \in \mathbb{R}^{l \times k}$.

**Output data:** Extended transition matrix $T$.

The proposed **Algorithm 1** allows finding such a linear operator $T$ which is not just a statistical regression but a structural bridge between two models. Let us consider the key aspects of its implementation:

1. **Formation of generators $J_i$:**
    The success of the algorithm depends on the correct determination of group action representation matrices. In practice, these matrices are calculated via local linearization of the feature space under small input data perturbations (e.g., image rotation by angle $\varepsilon \to 0$). This allows obtaining infinitesimal generators that describe exactly how FM and MM features change under the influence of a specific transformation.

2. **Role of weight coefficient $\lambda$:**
    Coefficient $\lambda$ regulates the compromise between two goals:
    * With **small $\lambda$**, the algorithm prioritizes approximation accuracy (Fidelity), making model $T$ maximally close to current data but less robust to geometric changes.
    * With **large $\lambda$**, priority is given to preserving structure (Equivariance). Matrix $T$ becomes more "physically correct," improving the generalization of explanations to new data that has undergone transformations.

3. **Computational stability and SVD:**
    Using SVD decomposition in Step 4 instead of direct matrix inversion (such as the least squares method via normal equations) ensures the algorithm's stability to multicollinearity. Since deep neural network features often correlate with each other, matrix $M$ can be close to degenerate. Singular decomposition allows effectively ignoring noisy data components by zeroing out inverse values for singular numbers lower than threshold $\tau$.

4. **Practical application:**
    After calculating matrix $T$, explaining DL model results becomes a trivial operation. For any new deep feature vector $a$, the corresponding vector of interpreted features $b^*$ is calculated as $b^* = Ta^*$. This allows translating the state of the "black box" into human-understandable parameters in real-time.

### 3.4. Numerical Example and Comparative Analysis on Synthetic Data

To demonstrate the advantages of the proposed method, we conduct a computational experiment on a synthetic dataset where the relationship between the formal (FM) and mental (MM) models is given explicitly.

#### 3.4.1. Experiment Setup

Consider a set of $m=15$ samples divided into three classes (analogous to that presented in [4], Appendix 1.1). Let:

* Matrix $A \in \mathbb{R}^{15 \times 5}$ represent deep features (Fig. 1 (a)).
* Matrix $B \in \mathbb{R}^{15 \times 4}$ represent interpreted parameters (Fig. 1 (b)).
* Symmetry group – $SO(2)$ (rotations), where generators $J^A$ and $J^B$ correspond to rotation in the respective feature subspaces.

##### (a) Formal Model

*Figure 1 (a). Visualization using MDS: matrix $A \in \mathbb{R}^{15 \times 5}$. (Scatter plot showing 3 classes)*

##### (b) Mental (Human) Model

*Figure 1 (b). Visualization using MDS: matrix $B \in \mathbb{R}^{15 \times 4}$. (Scatter plot showing 3 classes)*

Next, we obtain generators $J$. The mathematical logic is as follows.

Generator $J$ describes the rate of feature change. If we slightly transform the input (e.g., rotate the input image by angle $\epsilon$), the new features $A_\epsilon$ will be related to the old $A$ via the generator: $A_\epsilon \approx A(I + \epsilon J^\top)$. From this, we obtain the equation for $J$: $AJ^\top \approx (A_\epsilon - A)/\epsilon$. Let $\Delta A = (A_\epsilon - A)/\epsilon$ (this is the numerical derivative of features with respect to the transformation parameter). Then the problem reduces to: $A J^\top = \Delta A$.

We model $\Delta A$, assuming that "rotation" in the feature space is a linear combination of existing features.

Given that in this example we have only synthetically created matrices $A$ and $B$ and do not have input data and functions (deep learning models and machine learning models), an approach is proposed that effectively allows "grounding" abstract synthetic data by giving them geometric meaning through visualization. Since matrices $A$ and $B$ have no connection to input images, we use 2D space as a "control bridge".

For this, we need an inverse mapping. Since standard dimensionality reduction methods (t-SNE or MDS) do not have a direct inverse formula, we build it ourselves using regression.

##### Algorithm 2. Obtaining generator $J$ via 2D rotation

1. **Reduction:** Translate $A \in \mathbb{R}^{15 \times 5}$ into $A_{2D} \in \mathbb{R}^{15 \times 2}$ using the MDS method (as this method is best suited for this, preserving global distances).
2. **Bridge Construction (Inverse Map):** Train a linear regression (decoder) that maps $(x, y) \to (a_1, ..., a_5)$.
3. **Rotation:** Rotate points in 2D by a small angle $\epsilon$ and obtain $A_{2D, rot}$.
4. **Return:** Pass $A_{2D, rot}$ through the decoder to obtain $A_{rot}$ in the 5-dimensional space.
5. **Generator:** Using existing $A$ and $A_{rot}$, calculate $J_A$ via $\Delta A$.

**App. 1.2.** provides basic Python code for obtaining generator $J$.

Similarly, we obtain $J_B$ for matrix $B$. Since both matrices ($A$ and $B$) originate from the same objects, their 2D projections will be similar, and we will find a connection between them via **Algorithm 2**.

As a result, we obtain matrices $J^A (5 \times 5)$ and $J^B (4 \times 4)$ (**App. 1.2.**), which are ideally suited for the intertwining equation $T J_A - J_B T$. In summary, we can assert that matrix $T$ "respects" the rotation that a human sees on the visualization screen.

For the given numerical example, choose $r=1$ (one generator), which corresponds to the group $SO(2)$ (rotations on a plane). This choice stems from the following considerations: (1) Since we perform reduction to a 2-dimensional space (MDS), there exists only one fundamental type of continuous rotation—around the coordinate center. This transformation corresponds to exactly one generator in the Lie algebra. (2) This makes block matrices $M$ and $Y$ more compact and easier to understand.

Also, for this example, choose a very small angle $\epsilon = 0.01$ radians (or even 0.001). The choice of such small values is justified by the fact that the generator determines group action in the vicinity of unity (i.e., at zero rotation). If a large angle is taken, the linear approximation $A_{rot} \approx A(I + \epsilon J^\top)$ becomes inaccurate, and we obtain a "dirty" generator with a large error. Also note that there is no need to take different angles for different rows. Group action is a global operation. We rotate the entire point cloud in 2D by the same angle $\epsilon$. This allows us to find a single matrix $J$ describing this rotation for the entire feature space simultaneously.

#### 3.4.2. Scenario 1: Old Approach [4] (Static Transition Matrix)

Using the method described in [4], matrix $T_{old}$ was calculated (**App. 1.1.**). Using the obtained matrix, mental model parameters were reconstructed: $B^*_{old} = A T^\top_{old}$ (**App. 1.4.**). The result will be evaluated by metrics:

* **Approximation error** is calculated by the formula $MSE\_fid = \frac{1}{m \cdot l} \| B - B^*_{old} \|_F^2$ in the following way:
  * Step 1: Find the residual matrix (difference).
  * Step 2: Square each matrix element, sum all these squares (this is the square of the Frobenius norm).
  * Step 3: Divide the sum by the total number of elements ($m \times l$) to get the mean value.
* **Symmetry defect** is calculated by the formula $Sym\_err = \| T_{old} J^A - J^B T_{old} \|_F^2$ with the following steps:
  * Step 1: Calculate the left side.
  * Step 2: Calculate the right side.
  * Step 3: Find the defect matrix.
  * Step 4: Calculate the sum of squares of all matrix elements.

The value $MSE\_fid$ should be small (since $T$ was optimized specifically for this task). However, the value $Sym\_err$ should be quite large. Such computational values prove the mathematics presented above, namely that the old approach ignores the "geometry" of data, treating features simply as a set of numbers, not as equivariant representations.

In Scenario 2, we will show that by slightly sacrificing $MSE\_fid$, we can radically reduce $Sym\_err$, making the model robust to transformations.

#### 3.4.3. Scenario 2: New Approach (Equivariant Transition Matrix)

Apply the extended algorithm (Section 3.3) with weight coefficient $\lambda = 0.5$. Calculate matrix $T_{new}$ and its corresponding prediction matrix $B^*_{new} = A T^\top_{new}$ (**App. 1.4**).

Evaluate the obtained result using formulas $MSE\_fid$ and $Sym\_err$.

#### 3.4.4. Scenario 3: Robustness Test to Transformations

The most critical test for XAI models is checking the stability of explanations under input data variations. For this scenario, we generate a new "test" feature matrix $A_{rot}$ that mimics the rotation of original objects. Since in the synthetic example there is no direct link with input images, we implement this step via the geometric visualization space (MDS), using **Algorithm 2**:

1. For each sample $j$ in visualization space $A_{2D}$, perform rotation by angle $\alpha_i \in [-\pi/6; \pi/6]$, corresponding to $\pm 30^\circ$ with a step of $5^\circ$.
2. Using the trained "inverse bridge" (decoder from Algorithm 2), the obtained 2D points are mapped back to the 5-dimensional FM feature space, forming matrix $A_{rot}$.
3. For the obtained matrix $A_{rot}$, calculate interpreted feature predictions in two ways:
    * Old approach: $B^*_{old\_rot} = A_{rot} T^\top_{old}$.
    * New approach: $B^*_{new\_rot} = A_{rot} T^\top_{new}$.

To assess robustness, compare the obtained vectors $b^*$ with "ideal" values $b_{target}$ that should have resulted if the mental model MM had also ideally rotated by angle $\alpha$. Results of MSE comparison on rotated data are given in the last row of **Table 1**.

By the logic of mathematical operations, the old matrix $T_{old}$ should ignore symmetry structure and demonstrate high error, indicating a "chaotic" change in explanations upon object rotation. Conversely, the new matrix $T_{new}$ should preserve structural connection, demonstrating low error and ensuring interpretation consistency.

#### 3.4.5. Conclusions of the Experiment

The explanation consistency metric (i.e., correspondence between rotation in FM space and rotation in MM space) should show that $B^*_{old}$ predictions demonstrate chaotic deviations: the model "loses" the link between parameters when the object changes position, whereas $B^*_{new}$ predictions should demonstrate stable equivariance: a change in deep features translates into a logically predictable change in interpreted parameters (**Table 1**).

##### Table 1. Template for presenting experiment results on synthetic data

| Metric | Old Approach (Fidelity-only) | New Approach (Equivariant) |
| :--- | :--- | :--- |
| MSE on training data | – | – |
| Symmetry defect (Sym_err) | – | – |
| Error on rotated data | – | – |

Thus, the numerical example should confirm that accounting for infinitesimal Lie group generators allows obtaining a transition matrix robust to input signal transformations. Although such a transition matrix may lead to a slight increase in error on the static sample, it ensures orders of magnitude higher stability of explanations on new, modified data, which is a critical requirement for trust in explainable artificial intelligence systems.

> **[Author Note]**
> To the final results, a graph (Scatter Plot) must be added to section 3.4, visually showing vectors $b^*$ for the old and new methods on rotated data. This visualization should show the contrast of the "chaos" of the old method compared to the "orderliness" of the new method.
> Two Scatter Plots must be made side by side:
>
> * Left: Result $B^*_{old}$ on rotated data: show that points mixed up or flew out of clusters.
> * Right: Result $B^*_{new}$ on rotated data: show that cluster structure was preserved.
>
> This is visual proof of the "Robustness Test" (Scenario 3).
>
> Also, such a graph must be added:
>
> * On it, points $B^*_{old\_rot}$ will look like "scattered grain" (loss of cluster structure).
> * Points $B^*_{new\_rot}$ will look like distinct clusters, just slightly shifted with structure preservation.
>
> ***
> **[Implementation Comment]:**
> (See text block above regarding implementation details).

### 3.5. Experimental Setup

To evaluate the effectiveness of the proposed approach and compare it with the classical transition matrix construction methodology [4], a series of computational experiments on the benchmark MNIST dataset must be conducted.

#### 3.5.1. Description of Dataset and Models

The MNIST dataset consists of 70,000 images of handwritten digits sized 28×28 pixels in grayscale. For the experiment, the standard split into training (60,000) and test (10,000) samples was used.

* **Formal Model (FM):** A convolutional neural network (CNN) with an architecture similar to that described in [4] is used as the deep model. The deep feature vector $a(x)$ is extracted from the penultimate (fully-connected) layer of the network, dimensionality of deep feature space $k = 490$.
* **Mental Model (MM):** To form interpreted features, images are unrolled into one-dimensional vectors of pixel intensities. Dimensionality of mental space $l = 784$.

#### 3.5.2. Experiment Methodology

The experimental study consists of three key stages:

1. **Baseline Level:** A static transition matrix $T_{old}$ is calculated according to the method described in [4], where only the reproduction accuracy functional (Fidelity) is optimized.
2. **Group Action and Generators:** To implement the equivariant approach, the action of the rotation group $SO(2)$ is introduced. The training sample is augmented with copies of images modified by a small angle $\epsilon = 0.01$ rad. Based on the reaction of CNN layers and pixel representation, infinitesimal generator matrices $J^A$ and $J^B$ are calculated.
3. **Calculation of Extended Matrix:** Using **Algorithm 1**, matrix $T_{new}$ is built, integrating approximation and symmetry conditions. The system solution is carried out via SVD decomposition with regularization parameter $\lambda$.

#### 3.5.3. Evaluation of Results and Metrics

To compare the baseline model and the proposed equivariant method, the following indicators are used:

* **Reconstruction Quality:** Evaluated based on Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR) metrics between original images and digits reconstructed from FM space via the transition matrix.
* **Symmetry Error:** Direct measurement of the violation of the intertwining equation $\| T J^A - J^B T \|_F$.
* **Robustness to Transformations:** Testing is conducted on a distorted (rotated) test sample. The ability of matrices $T_{old}$ and $T_{new}$ to preserve semantic content under geometric changes of input data is compared.

It is expected that using an equivariant transition matrix will ensure more stable results under input data variations, while preserving high visual reconstruction fidelity characteristic of the transition matrix method.

## 4. Results

[Content to be added]

## 5. Discussion

[Content to be added]

## Conclusions

[Content to be added]

## References

1. Miller, T. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 2019, 267, 1–38. <https://doi.org/10.1016/j.artint.2018.07.007>
2. Adadi, A.; Berrada, M. Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). IEEE Access 2018, 6, 52138–52160. <https://doi.org/10.1109/ACCESS.2018.2870052>
3. Guidotti, R.; Monreale, A.; Ruggieri, S.; Turini, F.; Giannotti, F.; Pedreschi, D. A Survey of Methods for Explaining Black Box Models. ACM Computing Surveys 2018, 51, 1–42. <https://doi.org/10.1145/3236009>
4. Radiuk P, Barmak O, Manziuk E, Krak I. Explainable Deep Learning: A Visual Analytics Approach with Transition Matrices. Mathematics. 2024; 12(7):1024. <https://doi.org/10.3390/math12071024>
5. Toward explainable deep learning in healthcare through transition matrix and user-friendly features / O. Barmak et a. Frontiers in Artificial Intelligence. 2024. Vol. 7. P. 1482141. URL: <https://doi.org/10.3389/frai.2024.1482141>
6. Bronstein, M.M.; Bruna, J.; Cohen, T.; Veličković, P. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. arXiv preprint 2021, arXiv:2104.13478. <https://doi.org/10.48550/arXiv.2104.13478>
7. Cohen, T.; Welling, M. Group Equivariant Convolutional Networks. Proceedings of the 33rd International Conference on Machine Learning (ICML) 2016, 48, 2990–2999.
8. Finzi, M.; Stanton, S.; Izmailov, P.; Wilson, A.G. Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Manifolds. Proceedings of the 37th International Conference on Machine Learning (ICML) 2020, 119, 3165–3176.
9. Akritas, A.G.; Malaschonok, G.I. Applications of Singular-Value Decomposition (SVD). Mathematics and Computers in Simulation 2004, 67, 15–31, doi:10.1016/j.matcom.2004.05.005.
10. Michael Galkin. Graph & Geometric ML in 2024: Where We Are and What's Next (Part I — Theory & Architectures). Towards Data Science, Medium, 2024. <https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1>
11. Michael Galkin. Graph & Geometric ML in 2024: Where We Are and What's Next (Part II — Applications). Towards Data Science, Medium, 2024. <https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63>
12. Crocioni et al. DeepRank2: Mining 3D Protein Structures with Geometric Deep Learning. Journal of Open Source Software, 9(94), 5983, 2024. <https://doi.org/10.21105/joss.05983>
13. Dvorkin et al. Geometric deep learning framework for de novo genome assembly. Genome Research, 35(4):839–849, 2025. <https://doi.org/10.1101/gr.279307.124>
14. He et al. A deep equivariant neural network approach for efficient hybrid density functional calculations. Nature Communications, 15, Article number: 8690, 2024. <https://doi.org/10.1038/s41467-024-53028-4>
15. Batatia et al. A General Framework for Equivariant Neural Networks on Reductive Lie Groups. NeurIPS 2023. <https://arxiv.org/abs/2306.00018> (accepted paper)
16. M. Geiger et al. The principles behind equivariant neural networks for physics and chemistry. PNAS, 122(2), e2415656122, 2025. <https://doi.org/10.1073/pnas.2415656122>
17. Claudio Battiloro et al. E(n) Equivariant Topological Neural Networks. ICLR 2025 Poster. <https://openreview.net/forum?id=Ax3uliEBVR>
18. M. K. Maurer et al. Equivariant neural networks for robust observables. Phys. Rev. D 110, 096023, 2024. <https://doi.org/10.1103/PhysRevD.110.096023>
19. Ghorbel et al. Equivariant and SE(2)-Invariant Neural Network Leveraging Fourier-Based Descriptors for 2D Image Classification. Proceedings of the 17th International Conference on Agents and Artificial Intelligence (ICAART 2025), Volume 2, pages 210-215, 2025. <https://doi.org/10.5220/0013143300003890>
20. Navon et al. Equivariant Architectures for Learning in Deep Weight Spaces. arXiv:2301.12780, 2023.
21. Pacini et al. On Universality Classes of Equivariant Networks. arXiv:2506.02293, 2025.
22. J. Yim et al. SE(3) Diffusion Model with Application to Protein Backbone Generation. arXiv:2302.02277, 2023 (FrameDiff).
23. S. Alamdari et al. Protein generation with evolutionary diffusion: sequence is all you need. bioRxiv, 2023. <https://doi.org/10.1101/2023.09.11.556673> (EvoDiff).
24. K. Martinkus et al. AbDiffuser: Full-Atom Generation of in-vitro Functioning Antibodies. arXiv:2308.05027, 2023.
25. E. Sverrison et al. DiffMaSIF: Surface-based Protein-Protein Docking with Diffusion Models. MLSB 2023.
26. M. Ketata et al. DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models. arXiv:2304.03889, 2023.
27. Z. Zhang et al. DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing. arXiv:2306.01794, 2023.
28. R. Krishna et al. Generalized Biomolecular Modeling and Design with RoseTTAFold All-Atom. bioRxiv, 2023. <https://doi.org/10.1101/2023.10.09.561603>

***

## Appendix 1. Data for Section 3.4

### Appendix 1.1. Matrices $A \in \mathbb{R}^{15 \times 5}, B \in \mathbb{R}^{15 \times 4}, T_{old} \in \mathbb{R}^{5 \times 4}$

$$
A =
\begin{pmatrix}
2.8 & -1.8 & -2.8 & 1.3 & 0.4 \\
2.9 & -1.9 & -2.9 & 1.4 & 0.5 \\
3 & -2 & -3 & 1.5 & 0.6 \\
3.1 & -2.1 & -3.1 & 1.6 & 0.7 \\
3.2 & -2.2 & -3.2 & 1.7 & 0.8 \\
-1.6 & -2.5 & 1.5 & 0.2 & 0.6 \\
-1.3 & -2.7 & 1.3 & 0.4 & 0.8 \\
-1 & -3 & 1.5 & 0.6 & 1 \\
-0.7 & -3.2 & 1.7 & 0.8 & 1.2 \\
-0.5 & -3.5 & 1.9 & 1 & 1.4 \\
1.2 & -1.2 & 0.7 & -0.3 & -2.8 \\
1.1 & -1.1 & 0.8 & -0.4 & -2.9 \\
1 & -1 & 0.8(4) & -0.(4) & -3 \\
0.9 & -0.9 & 0.85 & -0.45 & -3.1 \\
0.8 & -0.8 & 0.9 & -0.5 & -3.2
\end{pmatrix},
$$

$$
B =
\begin{pmatrix}
-1.979394104 & 1.959307524 & -1.381119943 & -1.72964 \\
-1.974921385 & 1.94850558 & -1.726609792 & -1.76121 \\
-1.843907868 & 1.99818664 & -1.912855282 & -1.97511 \\
-1.998625355 & 1.999671808 & -1.998443276 & -1.99976 \\
-1.999365095 & 1.998896097 & -1.999605076 & -1.99892 \\
1.997775859 & -1.844000202 & 1.660111333 & -1.37353 \\
1.818753218 & -1.909687734 & 1.206631506 & -1.40799 \\
1.992023578 & -1.923804827 & 0.706593926 & -1.54378 \\
1.999174385 & -1.997592083 & 0.21221635 & -1.58697 \\
1.997854305 & -1.999410881 & -0.243400633 & -1.82759 \\
0.851626415 & 1.574201387 & 1.581026838 & 1.573934 \\
1.008512576 & 1.570791652 & 1.595657199 & 1.741762 \\
1.107744254 & 1.615475549 & 1.723582196 & 1.807615 \\
1.089897991 & 1.611369928 & 1.882537367 & 1.873522 \\
1.290406093 & 1.695289797 & 1.953503509 & 1.94625
\end{pmatrix},
$$

$$
T_{old} =
\begin{pmatrix}
-0.278135369 & 0.520567817 & -0.140387778 & 0.024426 \\
-0.382248581 & 0.126035484 & -0.145008015 & 0.349038 \\
0.522859856 & -0.341076002 & 0.433255464 & 0.198781 \\
-0.065904355 & -0.023301678 & -0.149755201 & -0.25589 \\
-0.177604706 & -0.49953555 & -0.428847974 & -0.61688
\end{pmatrix}
$$

### Appendix 1.2. Definition of $J$

```python
codePython
import numpy as np
from sklearn.manifold import MDS
from sklearn.linear_model import LinearRegression

# 1. Matrix A (example)
A = np.array([...])

# 2. Reduction to 2D (MDS)
mds = MDS(n_components=2, random_state=42, normalized_stress=False)
A_2d = mds.fit_transform(A)

# 3. Train "inverse bridge" (Decoder: 2D -> 5D)
decoder = LinearRegression()
decoder.fit(A_2d, A)

# 4. Perform small rotation in 2D
epsilon = 0.01 # small angle in radians
R = np.array([
 [np.cos(epsilon), -np.sin(epsilon)],
 [np.sin(epsilon), np.cos(epsilon)]
])
A_2d_rot = A_2d @ R.T

# 5. Return to 5D (Inverse Mapping)
A_rot = decoder.predict(A_2d_rot)

# 6. Calculate delta_A (numerical derivative)
delta_A = (A_rot - A) / epsilon

# 7. Find generator J_A (5x5)
# Equation: A * J.T = delta_A
J_A_T = np.linalg.pinv(A) @ delta_A
J_A = J_A_T.T

print("Calculated generator J_A (5x5):")
print(np.round(J_A, 4))
```

### Appendix 1.3. Matrices $J^A (5 \times 5)$ and $J^B (4 \times 4)$

$$
J^A =
\begin{pmatrix}
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & &
\end{pmatrix},
$$

$$
J^B =
\begin{pmatrix}
 & & & \\
 & & & \\
 & & & \\
 & & &
\end{pmatrix}
$$

### Appendix 1.4. Matrix $B^*_{old}, T_{new}, B^*_{new}$

$$
B^*_{old} =
\begin{pmatrix}
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & &
\end{pmatrix},
$$

$$
T_{new} =
\begin{pmatrix}
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & &
\end{pmatrix},
$$

$$
B^*_{new} =
\begin{pmatrix}
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
 & & &
\end{pmatrix}
$$

### Appendix 1.5 Matrix $A_{rot}$

$$
A_{rot} =
\begin{pmatrix}
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & & \\
 & & & &
\end{pmatrix}
$$
